{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Text Detection Model Training for Vertex AI\n",
    "\n",
    "This notebook implements a fine-tuning pipeline for RoBERTa model using LoRA (Low-Rank Adaptation) for AI-generated text detection, adapted for Google Cloud Vertex AI training.\n",
    "\n",
    "## Overview\n",
    "1. Setup Project Structure\n",
    "2. Create Training Package\n",
    "3. Upload to Google Cloud Storage\n",
    "4. Configure and Launch Vertex AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we'll install the Google Cloud SDK and other required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform>=2.11.0 google-cloud-storage>=2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up Google Cloud Project\n",
    "\n",
    "Configure your Google Cloud project and create necessary directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Google Cloud project ID and region\n",
    "PROJECT_ID = \"your-project-id\"  # Replace with your project ID\n",
    "REGION = \"us-central1\"  # Replace with your desired region\n",
    "BUCKET_NAME = \"your-bucket-name\"  # Replace with your GCS bucket name\n",
    "\n",
    "# Create trainer package directory structure\n",
    "!mkdir -p trainer\n",
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Training Module\n",
    "\n",
    "Create the main training module (trainer/task.py) that will be executed by Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-dir', type=str, required=True)\n",
    "    parser.add_argument('--train-data', type=str, required=True)\n",
    "    parser.add_argument('--valid-data', type=str, required=True)\n",
    "    parser.add_argument('--epochs', type=int, default=3)\n",
    "    parser.add_argument('--batch-size', type=int, default=8)\n",
    "    return parser.parse_args()\n",
    "\n",
    "def train_model(args):\n",
    "    # Label configuration\n",
    "    label2id = {\"HUMAN\": 0, \"AI\": 1}\n",
    "    id2label = {0: \"HUMAN\", 1: \"AI\"}\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"roberta-base\"\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label2id),\n",
    "        label2id=label2id,\n",
    "        id2label=id2label\n",
    "    )\n",
    "\n",
    "    # Load datasets\n",
    "    tokenized_training_dataset = load_from_disk(args.train_data)\n",
    "    tokenized_validation_dataset = load_from_disk(args.valid_data)\n",
    "\n",
    "    # Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.model_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        per_device_eval_batch_size=args.batch_size,\n",
    "        num_train_epochs=args.epochs,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"tensorboard\"\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_training_dataset,\n",
    "        eval_dataset=tokenized_validation_dataset\n",
    "    )\n",
    "\n",
    "    # Train and save\n",
    "    trainer.train()\n",
    "    peft_model.save_pretrained(args.model_dir)\n",
    "    tokenizer.save_pretrained(args.model_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    train_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create setup.py\n",
    "\n",
    "Create the setup.py file for packaging the training application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'datasets>=2.15.0',\n",
    "    'torch>=2.1.0',\n",
    "    'transformers>=4.36.0',\n",
    "    'numpy>=1.24.0,<2.0.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'accelerate>=0.23.0',\n",
    "    'peft>=0.6.0'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='ai_text_detection_trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='AI Text Detection training application for Vertex AI'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and Upload Training Package\n",
    "\n",
    "Build the training package and upload it to Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the package\n",
    "!python setup.py sdist --formats=gztar\n",
    "\n",
    "# Upload to GCS\n",
    "!gsutil cp dist/ai_text_detection_trainer-0.1.tar.gz gs://$BUCKET_NAME/trainer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch Vertex AI Training Job\n",
    "\n",
    "Configure and launch the training job on Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Configure the training job\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"ai-text-detection-training\",\n",
    "    script_path=\"trainer/task.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latest\",\n",
    "    requirements=[f\"gs://{BUCKET_NAME}/trainer/ai_text_detection_trainer-0.1.tar.gz\"],\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1\n",
    ")\n",
    "\n",
    "# Launch the training job\n",
    "job.run(\n",
    "    args=[\n",
    "        f\"--model-dir=gs://{BUCKET_NAME}/model_output\",\n",
    "        f\"--train-data=gs://{BUCKET_NAME}/data/tokenized_training\",\n",
    "        f\"--valid-data=gs://{BUCKET_NAME}/data/tokenized_validation\",\n",
    "        \"--epochs=3\",\n",
    "        \"--batch-size=8\"\n",
    "    ],\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After the training job completes:\n",
    "1. The trained model will be saved in your specified GCS bucket\n",
    "2. You can download and evaluate the model locally\n",
    "3. Deploy the model to a Vertex AI endpoint for predictions\n",
    "4. Monitor the model's performance in production\n",
    "\n",
    "Remember to clean up resources when they're no longer needed to avoid unnecessary charges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
