{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# AI Text Detection Model Training\n",
       "\n",
       "This notebook implements a fine-tuning pipeline for RoBERTa model using LoRA (Low-Rank Adaptation) for AI-generated text detection. The model will be trained to classify text as either human-written or AI-generated.\n",
       "\n",
       "## Overview of the Training Process\n",
       "1. Setup and Dependencies Installation\n",
       "2. Data Preparation and Tokenization\n",
       "3. Model Configuration with LoRA\n",
       "4. Training Process\n",
       "5. Model Saving and Export\n",
       "\n",
       "This approach uses Parameter-Efficient Fine-Tuning (PEFT) to reduce the computational resources needed while maintaining model performance."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Dependencies Installation\n",
       "\n",
       "First, we'll install all required packages. These include:\n",
       "- `transformers`: For the RoBERTa model and tokenizer\n",
       "- `datasets`: For efficient data handling\n",
       "- `peft`: For Parameter-Efficient Fine-Tuning\n",
       "- Supporting libraries for deep learning and data processing"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "!pip install datasets>=2.15.0 python-dotenv>=1.0.0 torch>=2.1.0 transformers>=4.36.0 \\\n",
       "    numpy>=1.24.0,<2.0.0 pandas>=2.0.0 hf-xet>=1.1.2 accelerate>=0.23.0 peft>=0.6.0"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Import Required Libraries\n",
       "\n",
       "We import the necessary components:\n",
       "- `RobertaTokenizerFast`: Fast tokenizer implementation for RoBERTa\n",
       "- `RobertaForSequenceClassification`: Pre-trained RoBERTa model adapted for classification\n",
       "- `Trainer` and `TrainingArguments`: HuggingFace's training utilities\n",
       "- PEFT components for efficient fine-tuning"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
       "from datasets import load_from_disk\n",
       "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Label Configuration\n",
       "\n",
       "We define our label mappings for binary classification:\n",
       "- HUMAN (0): Represents human-written text\n",
       "- AI (1): Represents AI-generated text\n",
       "\n",
       "We create bidirectional mappings (`label2id` and `id2label`) for easy conversion between text labels and numeric indices."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "label2id = {\"HUMAN\": 0, \"AI\": 1}\n",
       "id2label = {0: \"HUMAN\", 1: \"AI\"}\n",
       "\n",
       "def map_labels_of_dataframe(frame):\n",
       "    \"\"\"Convert text labels to numeric indices\"\"\"\n",
       "    frame[\"label\"] = frame[\"label\"].map(label2id)\n",
       "    return frame"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Tokenizer and Dataset Preparation\n",
       "\n",
       "We use the RoBERTa tokenizer and load our pre-tokenized datasets. The tokenization process:\n",
       "- Uses maximum length padding\n",
       "- Applies truncation for longer sequences\n",
       "- Maintains consistency across all examples"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "model_name = \"roberta-base\"\n",
       "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
       "\n",
       "def tokenize_function(examples):\n",
       "    \"\"\"Tokenize text with padding and truncation\"\"\"\n",
       "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
       "\n",
       "# Load pre-processed datasets\n",
       "print(\"Loading tokenized datasets...\")\n",
       "tokenized_training_dataset = load_from_disk(\"data/tokenized_training\")\n",
       "tokenized_validation_dataset = load_from_disk(\"data/tokenized_validation\")\n",
       "print(f\"Training samples: {len(tokenized_training_dataset)}\")\n",
       "print(f\"Validation samples: {len(tokenized_validation_dataset)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Model Initialization\n",
       "\n",
       "We initialize the RoBERTa model for sequence classification with:\n",
       "- Binary classification setup (2 labels)\n",
       "- Proper label mappings\n",
       "- Base RoBERTa architecture"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "model = RobertaForSequenceClassification.from_pretrained(\n",
       "    model_name,\n",
       "    num_labels=len(label2id),\n",
       "    label2id=label2id,\n",
       "    id2label=id2label\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Training Configuration\n",
       "\n",
       "We set up the training arguments with:\n",
       "- Evaluation and saving strategies per epoch\n",
       "- Batch sizes optimized for memory efficiency\n",
       "- Weight decay for regularization\n",
       "- Logging configuration"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "output_dir = \"./results\"\n",
       "\n",
       "training_args = TrainingArguments(\n",
       "    output_dir=output_dir,\n",
       "    eval_strategy=\"epoch\",\n",
       "    save_strategy=\"epoch\",\n",
       "    logging_dir=f'{output_dir}/logs',\n",
       "    per_device_train_batch_size=8,\n",
       "    per_device_eval_batch_size=8,\n",
       "    num_train_epochs=3,\n",
       "    weight_decay=0.01,\n",
       "    report_to=\"none\"  # Disable wandb logging\n",
       ")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. LoRA Configuration\n",
       "\n",
       "We implement LoRA (Low-Rank Adaptation) for efficient fine-tuning:\n",
       "- Task type: Sequence Classification\n",
       "- Rank (r): 8 for parameter efficiency\n",
       "- Alpha: 32 for scaling\n",
       "- Dropout: 0.1 for regularization\n",
       "\n",
       "LoRA reduces the number of trainable parameters while maintaining model performance."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "peft_config = LoraConfig(\n",
       "    task_type=TaskType.SEQ_CLS,\n",
       "    inference_mode=False,\n",
       "    r=8,\n",
       "    lora_alpha=32,\n",
       "    lora_dropout=0.1\n",
       ")\n",
       "\n",
       "peft_model = get_peft_model(model, peft_config)\n",
       "# Display the number of trainable parameters\n",
       "peft_model.print_trainable_parameters()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Training Process\n",
       "\n",
       "We initialize the trainer with our LoRA-configured model and start the training process.\n",
       "The trainer will:\n",
       "- Handle batching and optimization\n",
       "- Perform evaluation after each epoch\n",
       "- Save checkpoints\n",
       "- Log training metrics"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "peft_lora_finetuning_trainer = Trainer(\n",
       "    model=peft_model,\n",
       "    args=training_args,\n",
       "    train_dataset=tokenized_training_dataset,\n",
       "    eval_dataset=tokenized_validation_dataset\n",
       ")\n",
       "\n",
       "print(\"Starting training...\")\n",
       "training_results = peft_lora_finetuning_trainer.train()\n",
       "print(\"Training completed!\")\n",
       "print(f\"Final training metrics: {training_results.metrics}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Save the Fine-tuned Model\n",
       "\n",
       "Finally, we save both the model and tokenizer for future use:\n",
       "- The model is saved with its LoRA adaptations\n",
       "- The tokenizer is saved to ensure consistent preprocessing in inference"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "model_output_dir = \"./finetuned_roberta\"\n",
       "print(f\"Saving model to {model_output_dir}...\")\n",
       "peft_model.save_pretrained(model_output_dir)\n",
       "tokenizer.save_pretrained(model_output_dir)\n",
       "print(\"Model and tokenizer saved successfully!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Next Steps\n",
       "\n",
       "After training, you can:\n",
       "1. Evaluate the model on a test set\n",
       "2. Use the model for inference on new text\n",
       "3. Analyze the model's performance metrics\n",
       "4. Fine-tune hyperparameters if needed\n",
       "\n",
       "The saved model can be loaded and used for inference using the same PEFT and transformers libraries."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }